{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickymaulanafajri/SPA/blob/main/SPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFEguGRX3dWe",
        "outputId": "5d9bac9c-4d0e-4239-d4aa-4a04b2b27843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_scatter-2.1.2%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt21cu118\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu118.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_sparse-0.6.18%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.3)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.23.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt21cu118\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu118.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_cluster-1.6.3%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.11.3)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.23.5)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt21cu118\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu118.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt21cu118-cp310-cp310-linux_x86_64.whl (887 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.8/887.8 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.2+pt21cu118\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.4.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g8IlWqG3rOm",
        "outputId": "463c0b9d-e923-4c2c-988e-06fb898d8957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cdlib\n",
            "  Downloading cdlib-0.3.0-py3-none-any.whl (230 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/230.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/230.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.3/230.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from cdlib) (1.23.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from cdlib) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from cdlib) (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from cdlib) (4.66.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from cdlib) (3.2.1)\n",
            "Collecting demon (from cdlib)\n",
            "  Downloading demon-2.0.6-py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: python-louvain>=0.16 in /usr/local/lib/python3.10/dist-packages (from cdlib) (0.16)\n",
            "Collecting nf1 (from cdlib)\n",
            "  Downloading nf1-0.0.4-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from cdlib) (1.11.3)\n",
            "Collecting pulp (from cdlib)\n",
            "  Downloading PuLP-2.7.0-py3-none-any.whl (14.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from cdlib) (0.12.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from cdlib) (1.5.3)\n",
            "Collecting eva-lcd (from cdlib)\n",
            "  Downloading eva_lcd-0.1.1-py3-none-any.whl (9.2 kB)\n",
            "Collecting bimlpa (from cdlib)\n",
            "  Downloading bimlpa-0.1.2-py3-none-any.whl (7.0 kB)\n",
            "Collecting markov-clustering (from cdlib)\n",
            "  Downloading markov_clustering-0.0.6.dev0-py3-none-any.whl (6.3 kB)\n",
            "Collecting python-igraph>=0.10 (from cdlib)\n",
            "  Downloading python_igraph-0.11.3-py3-none-any.whl (9.1 kB)\n",
            "Collecting angelcommunity (from cdlib)\n",
            "  Downloading angelcommunity-2.0.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from cdlib) (1.8.0)\n",
            "Collecting dynetx (from cdlib)\n",
            "  Downloading dynetx-0.3.2-py3-none-any.whl (39 kB)\n",
            "Collecting thresholdclustering (from cdlib)\n",
            "  Downloading thresholdclustering-1.1-py3-none-any.whl (5.3 kB)\n",
            "Collecting python-Levenshtein (from cdlib)\n",
            "  Downloading python_Levenshtein-0.23.0-py3-none-any.whl (9.4 kB)\n",
            "Collecting igraph==0.11.3 (from python-igraph>=0.10->cdlib)\n",
            "  Downloading igraph-0.11.3-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting texttable>=1.6.2 (from igraph==0.11.3->python-igraph>=0.10->cdlib)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from angelcommunity->cdlib) (0.18.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from dynetx->cdlib) (4.4.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cdlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cdlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cdlib) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cdlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cdlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cdlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cdlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->cdlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->cdlib) (2023.3.post1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->cdlib) (4.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch->cdlib) (2.31.0)\n",
            "Collecting Levenshtein==0.23.0 (from python-Levenshtein->cdlib)\n",
            "  Downloading Levenshtein-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.1.0 (from Levenshtein==0.23.0->python-Levenshtein->cdlib)\n",
            "  Downloading rapidfuzz-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->cdlib) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->cdlib) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->cdlib) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->cdlib) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->cdlib) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->cdlib) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->cdlib) (2023.7.22)\n",
            "Installing collected packages: texttable, pulp, thresholdclustering, rapidfuzz, igraph, eva-lcd, dynetx, demon, python-igraph, Levenshtein, python-Levenshtein, nf1, markov-clustering, bimlpa, angelcommunity, cdlib\n",
            "Successfully installed Levenshtein-0.23.0 angelcommunity-2.0.0 bimlpa-0.1.2 cdlib-0.3.0 demon-2.0.6 dynetx-0.3.2 eva-lcd-0.1.1 igraph-0.11.3 markov-clustering-0.0.6.dev0 nf1-0.0.4 pulp-2.7.0 python-Levenshtein-0.23.0 python-igraph-0.11.3 rapidfuzz-3.5.2 texttable-1.7.0 thresholdclustering-1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install cdlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28-yyCsZ4IXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff99bd2d-41bb-4430-d53a-979ebc3306b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: to be able to use all crisp methods, you need to install some additional packages:  {'infomap', 'leidenalg', 'wurlitzer', 'graph_tool', 'bayanpy'}\n",
            "Note: to be able to use all crisp methods, you need to install some additional packages:  {'ASLPAw', 'pyclustering'}\n",
            "Note: to be able to use all crisp methods, you need to install some additional packages:  {'leidenalg', 'infomap', 'wurlitzer'}\n"
          ]
        }
      ],
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import json\n",
        "import codecs\n",
        "from timeit import default_timer as timer\n",
        "from copy import deepcopy\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import cluster\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "#import dgl\n",
        "import networkx as nx\n",
        "from networkx.algorithms.link_analysis.pagerank_alg import pagerank\n",
        "from networkx.algorithms.community.quality import modularity\n",
        "from networkx.utils.mapped_queue import MappedQueue\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
        "from torch_geometric.datasets import Planetoid, CoraFull, Coauthor,WikiCS,GitHub,HeterophilousGraphDataset\n",
        "#from ogb.nodeproppred import PygNodePropPredDataset\n",
        "\n",
        "from cdlib import algorithms\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRaOKPE44iFV"
      },
      "outputs": [],
      "source": [
        "class ActiveLearning:\n",
        "    \"\"\"\n",
        "    An active learning framework that...\n",
        "    * queries from an oracle;\n",
        "    * updates its known set,\n",
        "    * trains the GNN model, and\n",
        "    * evaluate the Macro F-1 score.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, model, seed, args):\n",
        "        self.round = 0\n",
        "        self.data = data\n",
        "        self.model = model\n",
        "        self.seed = seed\n",
        "        self.args = args\n",
        "        self.retrain = args.retrain\n",
        "        self.clf = None\n",
        "        self.aggregated = None\n",
        "        self.num_centers = args.num_centers\n",
        "        self.num_parts = -1\n",
        "\n",
        "    def query(self, b):\n",
        "        pass\n",
        "\n",
        "    def update(self, train_mask):\n",
        "        self.data.train_mask = train_mask\n",
        "        self.round += 1\n",
        "\n",
        "    def train(self):\n",
        "        if self.retrain:\n",
        "            self.clf = deepcopy(self.model).to(self.args.device)\n",
        "        else:\n",
        "            self.clf = self.model.to(self.args.device)\n",
        "        optimizer = optim.Adam(\n",
        "            self.clf.parameters(), lr=self.args.lr,\n",
        "            weight_decay=self.args.weight_decay)\n",
        "        for epoch in range(self.args.epochs):\n",
        "            self.clf.train()\n",
        "            optimizer.zero_grad()\n",
        "            out = self.clf(self.data.x, self.data.adj_t)\n",
        "            true = self.data.y\n",
        "            if len(true.shape) > 1:\n",
        "                true = true.squeeze(1)\n",
        "            loss = F.cross_entropy(\n",
        "                out[self.data.train_mask],\n",
        "                true[self.data.train_mask])\n",
        "            if self.args.verbose == 2:\n",
        "                print('Epoch {:03d}: Training loss: {:.4f}'.format(epoch, loss))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.clf.eval()\n",
        "        logits = self.clf(self.data.x, self.data.adj_t)\n",
        "        y_pred = logits.max(1)[1].cpu()\n",
        "        y_true = self.data.y.cpu()\n",
        "        f1 = metrics.f1_score(y_true, y_pred, average='macro')\n",
        "        acc = metrics.f1_score(y_true, y_pred, average='micro')\n",
        "        if self.args.verbose == 2:\n",
        "            print('Macro-f1 score: {:.4f}'.format(f1))\n",
        "            print('Micro-f1 score: {:.4f}'.format(acc))\n",
        "        return f1, acc\n",
        "\n",
        "    def get_node_representation(self, rep='aggregation', encoder='gcn'):\n",
        "\n",
        "        if rep == 'aggregation':\n",
        "            if self.aggregated is None:\n",
        "                feat_dim = self.data.x.size(1)\n",
        "                if encoder == 'sage':\n",
        "                    conv = SAGEConv(feat_dim, feat_dim, bias=False)\n",
        "                    conv.lin_l.weight = torch.nn.Parameter(torch.eye(feat_dim))\n",
        "                    conv.lin_r.weight = torch.nn.Parameter(torch.eye(feat_dim))\n",
        "                else:\n",
        "                    conv = GCNConv(feat_dim, feat_dim, cached=True, bias=False)\n",
        "                    conv.lin.weight = torch.nn.Parameter(torch.eye(feat_dim))\n",
        "                conv.to(self.args.device)\n",
        "                with torch.no_grad():\n",
        "                    self.aggregated = conv(self.data.x, self.data.adj_t)\n",
        "                    self.aggregated = conv(self.aggregated, self.data.adj_t)\n",
        "            return self.aggregated\n",
        "\n",
        "        elif rep == 'embedding':\n",
        "            with torch.no_grad():\n",
        "                embed = self.clf.embed(self.data.x, self.data.adj_t)\n",
        "            return embed\n",
        "\n",
        "        else:\n",
        "            return self.data.x\n",
        "\n",
        "    def split_cluster(self, b, partitions, x_embed=None, method='default'):\n",
        "\n",
        "        if method == 'inertia':\n",
        "            part_size = []\n",
        "            for i in range(self.num_parts):\n",
        "                part_id = np.where(partitions == i)[0]\n",
        "                x = x_embed[part_id]\n",
        "                kmeans = Cluster(n_clusters=1, n_dim=x_embed.shape[1], seed=self.seed, device=self.args.device)\n",
        "                kmeans.train(x.cpu())\n",
        "                inertia = kmeans.get_inertia()\n",
        "                part_size.append(inertia)\n",
        "\n",
        "            part_size = np.rint(b * np.array(part_size) / sum(part_size)).astype(int)\n",
        "            part_size = np.maximum(self.num_centers, part_size)\n",
        "            i = 0\n",
        "            while part_size.sum() - b != 0:\n",
        "                if part_size.sum() - b > 0:\n",
        "                    i = self.num_parts - 1 if i <= 0 else i\n",
        "                    while part_size[i] <= 1:\n",
        "                        i -= 1\n",
        "                    part_size[i] -= 1\n",
        "                    i -= 1\n",
        "                else:\n",
        "                    i = 0 if i >= self.num_parts else i\n",
        "                    part_size[i] += 1\n",
        "                    i += 1\n",
        "\n",
        "        elif method == 'size':\n",
        "            part_size = []\n",
        "            for i in range(self.num_parts):\n",
        "                part_size.append(len(np.where(partitions == i)[0]))\n",
        "            part_size = np.rint(b * np.array(part_size) / sum(part_size)).astype(int)\n",
        "            part_size = np.maximum(self.num_centers, part_size)\n",
        "            i = 0\n",
        "            while part_size.sum() - b != 0:\n",
        "                if part_size.sum() - b > 0:\n",
        "                    i = self.num_parts - 1 if i <= 0 else i\n",
        "                    while part_size[i] <= 1:\n",
        "                        i -= 1\n",
        "                    part_size[i] -= 1\n",
        "                    i -= 1\n",
        "                else:\n",
        "                    i = 0 if i >= self.num_parts else i\n",
        "                    part_size[i] += 1\n",
        "                    i += 1\n",
        "\n",
        "        else:\n",
        "            part_size = [b // self.num_parts for _ in range(self.num_parts)]\n",
        "            for i in range(b % self.num_parts):\n",
        "                part_size[i] += 1\n",
        "\n",
        "        return part_size\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Active Learning Agent (uninitialized)\"\n",
        "\n",
        "\n",
        "class Random(ActiveLearning):\n",
        "    \"\"\"\n",
        "    Random:\n",
        "    The Random Sampling method chooses nodes uniformly at random,\n",
        "    similarly as the commonly used semi-supervised learning experiment setting for GCN.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, model, seed, args):\n",
        "        super(Random, self).__init__(data, model, seed, args)\n",
        "\n",
        "    def query(self, b):\n",
        "        indice = np.random.choice(\n",
        "            np.where(self.data.train_mask == 0)[0], b, replace=False\n",
        "        )\n",
        "        return torch.tensor(indice)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Random\"\n",
        "\n",
        "\n",
        "\n",
        "class Uncertainty(ActiveLearning):\n",
        "    \"\"\"\n",
        "    Uncertainty:\n",
        "    The Uncertainty method chooses the nodes with maximum entropy on the predicted class distribution.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, model, seed, args):\n",
        "        super(Uncertainty, self).__init__(data, model, seed, args)\n",
        "\n",
        "    def query(self, b):\n",
        "        logits = self.clf(self.data.x, self.data.adj_t)\n",
        "        entropy = -torch.sum(F.softmax(logits, dim=1) * F.log_softmax(logits, dim=1), dim=1)\n",
        "        entropy[np.where(self.data.train_mask != 0)[0]] = 0\n",
        "        _, indices = torch.topk(entropy, k=b)\n",
        "        return indices\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Uncertainty\"\n",
        "\n",
        "\n",
        "#our proposed method\n",
        "class SCANPageRankOptimized(ActiveLearning):\n",
        "    \"\"\"\n",
        "    Sampling based on representativeness within communities using a dynamic approach with the SCAN algorithm for community detection. PageRank centrality, with the option for Personalized PageRank, is used as the metric for representativeness.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, model, seed, args, personalization=None, damping=0.95, max_iter=400, tol=1.0e-6, epsilon=0.5, mu=3):\n",
        "        super(SCANPageRankOptimized, self).__init__(data, model, seed, args)\n",
        "        self.personalization = personalization\n",
        "        self.damping = damping\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.epsilon = epsilon\n",
        "        self.mu = mu\n",
        "        self.previous_G = None\n",
        "\n",
        "    def compute_pagerank(self, G):\n",
        "        # Check if PageRank was already computed and if the graph has not changed\n",
        "        if not hasattr(self, 'pagerank_scores') or self.previous_G != G:\n",
        "            self.pagerank_scores = nx.pagerank(G, alpha=self.damping, personalization=self.personalization, max_iter=self.max_iter, tol=self.tol)\n",
        "            self.previous_G = G\n",
        "        return self.pagerank_scores\n",
        "\n",
        "    def query(self, b):\n",
        "        # Convert adjacency matrix to graph\n",
        "        adj_matrix = self.data.adj_t.to_dense().numpy()\n",
        "        G = nx.from_numpy_array(adj_matrix)\n",
        "\n",
        "        # Calculate PageRank for each node\n",
        "        pagerank_scores = self.compute_pagerank(G)\n",
        "\n",
        "        selected_indices = []\n",
        "        remaining_budget = b\n",
        "\n",
        "        while remaining_budget > 0:\n",
        "            # Apply the SCAN algorithm for community detection\n",
        "            partition = algorithms.scan(G, epsilon=self.epsilon, mu=self.mu)\n",
        "            communities = partition.communities\n",
        "\n",
        "            # Select nodes from each community based on PageRank scores\n",
        "            for community in communities:\n",
        "                if not community:\n",
        "                    continue\n",
        "                community_nodes = sorted([(node, pagerank_scores[node]) for node in community], key=lambda x: x[1], reverse=True)\n",
        "                num_samples = max(1, remaining_budget // len(communities))  # Ensure at least one sample per community\n",
        "                selected_indices.extend([node for node, _ in community_nodes[:num_samples]])\n",
        "                remaining_budget -= num_samples\n",
        "\n",
        "            # Break if no more selections are needed\n",
        "            if remaining_budget <= 0:\n",
        "                break\n",
        "\n",
        "        # Handle case where fewer nodes are selected than requested\n",
        "        if len(selected_indices) < b:\n",
        "            all_nodes_sorted = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "            remaining_indices = [node for node, _ in all_nodes_sorted if node not in selected_indices]\n",
        "            selected_indices.extend(remaining_indices[:b - len(selected_indices)])\n",
        "\n",
        "        # Convert to tensor\n",
        "        selected_indices = torch.tensor(selected_indices, dtype=torch.long)\n",
        "\n",
        "        return selected_indices[:b]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfwQN8Nl4sG2"
      },
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.5, batchnorm=False, activation=\"relu\"):\n",
        "        super(GCN, self).__init__()\n",
        "        assert activation in [\"relu\", \"elu\"]\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GCNConv(in_channels, hidden_channels, cached=True))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        if batchnorm:\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.num_layers = num_layers\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GCNConv(hidden_channels, hidden_channels, cached=True))\n",
        "            if batchnorm:\n",
        "                self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.convs.append(GCNConv(hidden_channels, out_channels, cached=True))\n",
        "        self.dropout = dropout\n",
        "        self.activation = getattr(F, activation)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        x = self.embed(x, adj_t)\n",
        "        x = self.convs[-1](x, adj_t)\n",
        "        return x\n",
        "\n",
        "    def embed(self, x, adj_t):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, adj_t)\n",
        "            if len(self.bns) > 0:\n",
        "                x = self.bns[i](x)\n",
        "            x = self.activation(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return x\n",
        "\n",
        "class SAGE(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    GraphSAGE\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels,\n",
        "                 num_layers=16, dropout=0.5, batchnorm=False, activation=\"relu\"):\n",
        "        super(SAGE, self).__init__()\n",
        "\n",
        "        assert activation in [\"relu\", \"elu\"]\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        if batchnorm:\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "            if batchnorm:\n",
        "                self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.activation = getattr(F, activation)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        x = self.embed(x, adj_t)\n",
        "        x = self.convs[-1](x, adj_t)\n",
        "        return x\n",
        "\n",
        "    def embed(self, x, adj_t):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, adj_t)\n",
        "            if len(self.bns) > 0:\n",
        "                x = self.bns[i](x)\n",
        "            x = self.activation(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwxC9Kf044EQ"
      },
      "outputs": [],
      "source": [
        "def run(data, args):\n",
        "\n",
        "    gnn = args.model\n",
        "    baseline = args.baselines\n",
        "    budget = int(args.budget)\n",
        "    seed = int(args.seed)\n",
        "\n",
        "    # Choose model\n",
        "    model_args = {\n",
        "        \"in_channels\": data.num_features,\n",
        "        \"out_channels\": data.num_classes,\n",
        "        \"hidden_channels\": args.hidden,\n",
        "        \"num_layers\": args.num_layers,\n",
        "        \"dropout\": args.dropout,\n",
        "        \"activation\": args.activation,\n",
        "        \"batchnorm\": args.batchnorm\n",
        "    }\n",
        "\n",
        "    # Initialize models\n",
        "    if gnn == \"gcn\":\n",
        "        model = GCN(**model_args)\n",
        "    elif gnn == \"sage\":\n",
        "        model = SAGE(**model_args)\n",
        "    else:\n",
        "        raise NotImplemented\n",
        "\n",
        "    model = model.to(args.device)\n",
        "\n",
        "    # Methods\n",
        "    if baseline == \"random\":\n",
        "        agent = Random(data, model, seed, args)\n",
        "    elif baseline == \"uncertainty\":\n",
        "        agent = Uncertainty(data, model, seed, args)\n",
        "    elif baseline == \"scanpage\":\n",
        "        agent = SCANPageRankOptimized(data,model,seed,args)\n",
        "       # Initialization\n",
        "    training_mask = np.zeros(data.num_nodes, dtype=bool)\n",
        "    initial_mask = np.arange(data.num_nodes)\n",
        "    np.random.shuffle(initial_mask)\n",
        "    init = args.init\n",
        "    if baseline in ['uncertainty']:\n",
        "        init = budget // 3\n",
        "    training_mask[initial_mask[:init]] = True\n",
        "\n",
        "    training_mask = torch.tensor(training_mask)\n",
        "    agent.update(training_mask)\n",
        "    agent.train()\n",
        "\n",
        "    if args.verbose > 0:\n",
        "        print('Round {:03d}: Labelled: {:d}, Prediction macro-f1 score {:.4f}'\n",
        "              .format(0, init, agent.evaluate()))\n",
        "\n",
        "    # Query\n",
        "    start = timer()\n",
        "    indices = agent.query(budget - init)\n",
        "    end = timer()\n",
        "    print('Total Query Runtime [s]:', end - start)\n",
        "\n",
        "    # Update\n",
        "    training_mask[indices] = True\n",
        "    agent.update(training_mask)\n",
        "\n",
        "    # Training\n",
        "    agent.train()\n",
        "\n",
        "    # Evaluate\n",
        "    f1, acc = agent.evaluate()\n",
        "    labelled = len(np.where(agent.data.train_mask != 0)[0])\n",
        "\n",
        "    if args.verbose > 0:\n",
        "        print('Round {:03d}: # Labelled nodes: {:d}, Prediction macro-f1 score {:.4f}'\n",
        "              .format(rd, labelled, f1))\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    return f1, acc, indices, agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsAYgZH048HW"
      },
      "outputs": [],
      "source": [
        "def test(data, gnns, budgets, baselines, seed=0):\n",
        "\n",
        "    # Set seeds\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"--verbose\", type=int, default=0, help=\"Verbose: 0, 1 or 2\")\n",
        "    parser.add_argument(\n",
        "        \"--device\", default=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "    # General configs\n",
        "    parser.add_argument(\n",
        "        \"--baselines\", type=str, default=baselines)\n",
        "    parser.add_argument(\n",
        "        \"--model\", default=gnns)\n",
        "    parser.add_argument(\n",
        "        \"--partition\", type=str, default='greedy')\n",
        "\n",
        "    # Active Learning parameters\n",
        "    parser.add_argument(\n",
        "        \"--budget\", type=int, default=budgets,\n",
        "        help=\"Number of rounds to run the agent.\")\n",
        "    parser.add_argument(\n",
        "        \"--retrain\", type=bool, default=True)\n",
        "    parser.add_argument(\n",
        "        \"--num_centers\", type=int, default=1)\n",
        "    parser.add_argument(\n",
        "        \"--representation\", type=str, default='aggregation')\n",
        "    parser.add_argument(\n",
        "        \"--compensation\", type=float, default=1.0)\n",
        "    parser.add_argument(\n",
        "        \"--init\", type=float, default=0, help=\"Number of initially labelled nodes.\")\n",
        "    parser.add_argument(\n",
        "        \"--epochs\", type=int, default=300, help=\"Number of epochs to train.\")\n",
        "    parser.add_argument(\n",
        "        \"--steps\", type=int, default=4, help=\"Number of steps of random walk.\")\n",
        "\n",
        "    # GNN parameters\n",
        "    parser.add_argument(\n",
        "        \"--seed\", type=int, default=seed, help=\"Number of random seeds.\")\n",
        "    parser.add_argument(\n",
        "        \"--lr\", type=float, default=0.01, help=\"Initial learning rate.\")\n",
        "    parser.add_argument(\n",
        "        \"--weight_decay\", type=float, default=5e-4,\n",
        "        help=\"Weight decay (L2 loss on parameters).\")\n",
        "    parser.add_argument(\n",
        "        \"--hidden\", type=int, default=16, help=\"Number of hidden units.\")\n",
        "    parser.add_argument(\n",
        "        \"--num_layers\", type=int, default=2, help=\"Number of layers.\")\n",
        "    parser.add_argument(\n",
        "        \"--dropout\", type=float, default=0,\n",
        "        help=\"Dropout rate (1 - keep probability).\")\n",
        "    parser.add_argument(\n",
        "        \"--batchnorm\", type=bool, default=False,\n",
        "        help=\"Perform batch normalization\")\n",
        "    parser.add_argument(\n",
        "        \"--activation\", default=\"relu\")\n",
        "\n",
        "    # GAT hyper-parameters\n",
        "    parser.add_argument(\n",
        "        \"--num_heads\", type=int, default=8, help=\"Number of heads.\")\n",
        "\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    f1_all, acc_all, queried, agent = run(data, args)\n",
        "\n",
        "    agg = agent.get_node_representation('aggregation', 'gcn').cpu().numpy()\n",
        "    agg_distance = {}\n",
        "    train_idx = list(queried.numpy())\n",
        "    test_idx = list(range(data.num_nodes))\n",
        "    train_agg = agg[train_idx]\n",
        "\n",
        "    for i in test_idx:\n",
        "        distance_tmp = train_agg - agg[i]\n",
        "        agg_distance[int(i)] = float(min(np.linalg.norm(distance_tmp, axis=1)))\n",
        "\n",
        "    group_num = 10\n",
        "    sort_res = list(\n",
        "        map(lambda x: x[0], sorted(agg_distance.items(), key=lambda x: x[1])))\n",
        "    node_num_group = len(sort_res) // group_num\n",
        "    res = [\n",
        "        sort_res[i:i + node_num_group + 1]\n",
        "        for i in range(0, len(sort_res), node_num_group + 1)\n",
        "    ]\n",
        "\n",
        "    acc_list = []\n",
        "    f1_list = []\n",
        "    agent.clf.eval()\n",
        "    logits = agent.clf(agent.data.x, agent.data.adj_t)\n",
        "    y_pred = logits.max(1)[1].cpu()\n",
        "    y_true = agent.data.y.cpu()\n",
        "    for test_set in res:\n",
        "        acc = metrics.accuracy_score(y_true[test_set], y_pred[test_set])\n",
        "        f1 = metrics.f1_score(y_true[test_set], y_pred[test_set], average='macro')\n",
        "        acc_list.append(acc)\n",
        "        f1_list.append(f1)\n",
        "    print(f\"Macro F1 {np.mean(f1_list)}\")\n",
        "    #print(f\"Accuracy {np.mean(acc_list)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Cn291-V4_kR",
        "outputId": "82cd9b26-9cef-40c7-b851-63ee4eeefef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3327\n",
            "9104\n",
            "6\n",
            "3703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "name = 'citeseer'\n",
        "\n",
        "path = os.path.join(\"data\", name)\n",
        "#cora, citeseer, pubmed using this Planetoid dataset class\n",
        "dataset = Planetoid(root=path, name=name, transform=T.ToSparseTensor())\n",
        "\n",
        "#use other dataset here\n",
        "\n",
        "data = dataset[0]\n",
        "data.max_part = 14\n",
        "data.num_classes = dataset.num_classes\n",
        "data.params = {'age': [0.35, 0.35, 0.3]}\n",
        "\n",
        "print(data.num_nodes)\n",
        "print(data.num_edges)\n",
        "print(data.num_classes)\n",
        "print(data.x.shape[1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AVDa4jB5C_X",
        "outputId": "81aef50c-a7c2-49ff-cb30-64437c3ac64c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Query Runtime [s]: 0.19624310200003947\n",
            "Macro F1 0.59155245280681\n"
          ]
        }
      ],
      "source": [
        "#change this part with 'gcn', or 'sage' for different graph architecture\n",
        "test(data,'gcn',40,'scanpage',seed=43)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsxg0f3L6h0u"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyN88/2BST03oc6uvriA6IKq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}